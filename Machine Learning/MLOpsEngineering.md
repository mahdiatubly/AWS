## Introduction to MLOps

* To implement quality control measures at the model registration step. If a model meets baseline performance metrics, it can be registered with a model registry. A model registry can be used as a quality gate. A model registry is a mechanism that is used to:
  * Catalog models for production.
  * Manage model versions.
  * Associate metadata, such as training metrics, with a model.
  * Manage the approval status of a model.
 

## Initial MLOps

* Deep Learning Containers provide optimized environments with TensorFlow and MXNet, Nvidia CUDA (for GPU instances), and Intel MKL (for CPU instances) libraries.
* SageMaker Studio environments do not support the elevated permissions needed to access the Docker daemon for building a Docker image. A separate build environment is needed to extend or build containers. The Amazon SageMaker Studio Image Build Command Line Interface (CLI) lets you build Amazon SageMaker-compatible Docker images directly from your Amazon SageMaker Studio environments.
* The build CLI automatically sets up a reusable build environment that you interact with by using high-level commands. The CLI orchestrates the build workflow using AWS CodeBuild and returns a link to your Amazon Elastic Container Registry (Amazon ECR) image location.
* Accessing the Docker daemon within SageMaker Studio:
  1. Assume execution role
  2. Package directory and upload to S3 bucket
  3. Build the Docker image using AWS CodeBuild
  4. Push the image to the ECR repository and return URI
 
* Training and inference containers in SageMaker must adhere to a specific folder structure. SageMaker-provided containers already have this folder structure in place. Any container that you extend or build new, must also follow this structure. A SageMaker model training container requires a parent directory named ‘/opt/ml’. Within that directory are subdirectories named: code, input, checkpoints, output, and model. SageMaker training uses environment variables to define storage paths.
  * code: You store the custom decision tree algorithm, ‘train.py,’ in this directory. The source container has a root directory containing a dockerfile, build_and_push.sh, and a subdirectory named ‘decision_trees’. This subdirectory contains the custom training algorithm, train.py
  * input: When you run a model training job, the SageMaker container uses SM_INPUT_DIR, which defaults to the /opt/ml/input/ directory. The JSON files that configure the hyperparameters for the algorithm and the resources used for distributed training are stored in the opt/ml/input/config directory. The input directory also contains a subdirectory within the /opt/ml/input/data directory for each channel of training data stored in Amazon S3.
  * checkpoints: If you use checkpointing, Amazon SageMaker saves checkpoints locally in the checkpoint_local_path, /opt/ml/checkpoints, and synchs them to the checkpoint_s3_uri.
  *  outputs: The outputs of a training job are sent from /opt/ml/output/data to the output data uri in S3 as output.tar.gz.
  *  The script must write the model generated by your algorithm to SM_MODEL_DIR, which defaults to /opt/ml/model/. When training is finished, the final model artifact in the /opt/ml/model folder is written to the output data URI in S3 as model.tar.gz.
  *  ../WORKDIR/: Training job operations that are distributed across multiple containers use the WORKDIR/.
 
* SageMaker downloads data from the S3 bucket to the container's file system. When the training job is complete, it uploads the trained model artifact from the local folder to the specified location in S3 and stops the training container.
* When creating your own custom containers for inference, SageMaker requires that you locate your inference code in specific directories. You can host a trained model for inference or batch transform. In both cases, the trained model is loaded into the container in the same folder to which they were written during training.
* You must also add the code required to perform inference using a web application. You place the following files in the /opt/ml/code directory:
   * serve.py – the program started when the container is created for hosting. It launches the gunicorn server which runs multiple instances of the web application defined in predictor.py.
   * predictor.py – the program that implements the web server and the decision tree predictions for this app.
   * webserver.conf – the configuration file for the front end.
   * wrapper.py – is a small wrapper used to invoke the web application.
 
* You can use pre-built containers to deploy your custom models or models that have been trained in a framework outside SageMaker:
  * First you convert the model into a format that is readable by the SageMaker model constructor for the corresponding framework.
  * Second, create a model in SageMaker Inference by pointing to model artifacts stored in Amazon S3 and a container image.
  * Finally, deploy the model to a SageMaker inference endpoint. You will need to choose the number of instances and the type of instances at which to deploy our model.
* The SageMaker Training Toolkit and the SageMaker Inference Toolkit. These two toolkits define the information the container needs to manage training and inference on Amazon SageMaker.
* Domain configurations: An Amazon SageMaker Domain consists of storage resources, a list of authorized users, and a variety of security, application, policy, and Amazon VPC configurations. Users within a Domain can share notebook files and other artifacts with each other. An account can have multiple Domains.
* User profile: A user profile represents a single user within a Domain. It is the main way to reference a user for the purposes of sharing, reporting, and other user-oriented features.
* Spaces: Spaces are used to manage the storage and resource needs of some Amazon SageMaker Studio applications. Each space has a 1:1 relationship with an instance of an application. Spaces can be either private or shared.
  * Private: Private spaces cannot be shared with other users.
  * Shared: Shared spaces are accessible by all users in the domain.
 
* Apps: An app represents an application that supports the reading and execution experience of the user’s notebooks, terminals, and consoles. The type of app can be JupyterServer, KernelGateway, RStudioServerPro, or RSession.
SageMaker offers various setup options optimized for different organizational needs:

1. **Quick Setup**
   - The fastest option to start using SageMaker Studio as an individual user.
   - Utilizes AWS Identity and Access Management (IAM) authentication with auto-generated defaults.
   - Helpful for single-user domains or first-time users exploring SageMaker.
   - Account configuration can be updated later.

2. **Standard Setup**
   - Requires the most configuration and offers the most options for customization.
   - Suitable for administrators managing large user groups.
   - Includes VPC configuration and setup for Single Sign-On or IAM Identity Provider.

3. **Templated SageMaker Domain in AWS Service Catalog**
   - Enables centralized management of AWS resources.
   - Data scientists can access the SageMaker Domain directly from a preconfigured environment through the AWS Service Catalog.
   - Ideal for organizations needing to comply with strict regulatory guidelines, such as HIPPA or Payment Card Industry (PCI).
   - Activates automated provisioning of environments preconfigured to comply with necessary policies.

* SageMaker JumpStart: Example notebooks and pre-trained models that you can use to fine-tune your own data.
* Notebook sharing: Share a snapshot of your notebook that team members can copy and work on their own version.  To share your latest version, you must create a new snapshot and then share it as a copy.
*  Shared spaces: A shared space consists of a shared JupyterServer application and a shared directory. Shared spaces facilitate collaboration in real-time. Co-editing is most effective for small groups. 
* SageMaker Projects: Templates you can use for orchestrating ML Ops and continuous integration and delivery (CI/CD) workflows. Orchestration includes managing dependencies, code repositories, build reproducibility, and artifact sharing.
* Encryption - SageMaker uses an AWS Key Management Service (AWS KMS) key to encrypt your Amazon EFS and Amazon Elastic Block Store (Amazon EBS) file systems by default. You can select a customer-managed key instead.
* Authentication – Choose between AWS IAM or AWS IAM Identity Center (successor to AWS SSO). Authenticate with AWS IAM to provide access to the Amazon SageMaker console or AWS CLI. You can use AWS IAM Identity Center to centralize identity management and provide a consistent user sign-in experience.
* Use Amazon SageMaker Role Manager to build and manage persona-based IAM roles for common machine learning needs directly through the Amazon SageMaker console. Amazon SageMaker Role Manager provides three preconfigured role personas with suggested permissions. Permissions are predefined for common ML activities. You can use the activities to create and maintain roles for personas unique to your business needs. These personas include the following:
  * Data Scientist persona – Use this persona to configure permissions to perform general machine learning development and experimentation in a SageMaker environment.
  * MLOps persona – Use this persona to configure permissions for operational activities.
  * SageMaker compute persona – Use this persona to configure permissions to your compute resources, such as the ability to interact with other AWS services.
 
* If the default environment does not meet your needs, consider using lifecycle configuration scripts. Lifecycle configurations are shell scripts triggered by SageMaker Studio lifecycle events, such as creating a new SageMaker Studio notebook or starting up a SageMaker Studio notebook. You can use these shell scripts to:
  * Install packages or sample notebooks on your notebook instance
  * Installing JupyterLab extensions
  * Preloading datasets
  * Configure git credentials with AWS Secrets Manager
  * Setting up source code repositories.
  * Running other shell scripts
  * Configure shutdown after idle timeout
  * To access AWS services from your notebook.
 
* Lifecycle configuration scripts can consist of up to 16384 characters and can run up to 5 minutes. To run a script as an OS process beyond the 5-minute timeframe, you can run the commands using nohup. Lifecycle configuration scripts log information to Amazon CloudWatch Logs under the under the log stream <DomainId>/<UserProfileName>/<AppType>/<AppName>. These logs are helpful for troubleshooting the lifecycle configuration job.
* Making experimentation environments self-service: AWS Service Catalog enables you to create and manage collections of logical IT products and services configured and parameterized as templates. These IT template products and services encompass various resources, including machine learning experimentation resources such as SageMaker Domains and SageMaker Studio User Profiles. With AWS Service Catalog, you can centrally manage commonly deployed IT services, facilitating consistent governance and compliance adherence. This platform empowers users to swiftly deploy only the approved IT services they require, streamlining operations and enhancing productivity.

## Repeatable MLOps
* Repository options on AWS:
  * AWS CodeCommit: A secure, highly scalable, fully managed source control service that hosts private Git repositories
  * Amazon SageMaker Feature Store: A fully managed, purpose-built repository to store, share, and manage features for machine learning (ML) models
  * Amazon SageMaker Model Registry: Catalogs and manages model versions
  * Third-party repository option: For example, GitHub and Bitbucket.
 
* ML workflow automation on AWS:
  * SageMaker provides a model-building pipeline through the SageMaker Pipelines SDK. With SageMaker Pipelines, you can create, automate, and manage end-to-end ML workflows at scale.
  * AWS Step Functions provide a serverless way to orchestrate pipelines, including ML-based ones.
  * Amazon Managed Workflows for Apache Airflow (MWAA) orchestrates your workflows by using Directed Acyclic Graphs (DAGs) written in Python.+
  * Third-party solutions include MLflow and Kubeflow. MLflow is an open-source platform to manage the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry. Kubeflow is a platform for building and deploying portable, scalable ML workflows, based on Docker containers. Kubeflow is the ML toolkit for Kubernetes.

* Amazon S3 provides versioning to protect data assets. When activated, Amazon S3 versioning will retain multiple copies of a data asset. When an asset is updated, prior versions of the asset will be retained and can be retrieved at any time. If an asset is deleted, the last version of it can be retrieved.
* SageMaker Data Wrangler, a visual data preparation tool. Data Wrangler is integrated into the SageMaker Studio UI. With the SageMaker Data Wrangler data selection tool, you can quickly access and select your tabular and image data from a wide variety of popular sources. For example, you can use Amazon S3, Amazon Athena, Amazon Redshift, AWS Lake Formation, Amazon EMR, Snowflake, and Databricks Delta Lake.
* Amazon AppFlow is a fully managed integration service. It helps you securely transfer data between software-as-a-service (SaaS) applications and AWS services such as Amazon S3 and Amazon Redshift.
* AWS Lake Formation data lake offers the features that are required to serve as the unified repository for all of your data securely. With a large number of Data Catalog resources, Lake Formation tag-based access control (LF-TBAC) is the recommended method to use to grant Lake Formation permissions. Principals must access data resources through an integrated service like SageMaker Data Wrangler.
* Features represent relevant attributes or properties that your model uses for training and to make predictions.
* With a feature store, you generate features by using raw data. Whenever the features are processed, the feature is committed to a central feature store, which serves as the centralized repository. Thus, it helps to eliminate redundancy in feature engineering pipelines and optimizes the ML development cycle. ML developers can easily search the centralized store to discover features to use for training or inference.
* SageMaker Feature Store organizes features in groups. You can think of a feature group as a data table and a feature as a column in the table. You can use the Amazon SageMaker Feature Store API or Amazon SageMaker Studio to add features to your feature group. When you add a feature to the feature group, you're effectively adding a column to the table. The features that you've added don't have any data. You can think of a record as a row in the data table. You can add new records to the feature group or overwrite them. Using the Feature Store, you can reconstruct all of the supporting features that are connected to that record as of the time of the timestamp. Therefore, the data is always time-consistent.

* Amazon SageMaker Feature Store Spark is a Spark connector that connects the Spark library to the Feature Store. Feature Store Spark simplifies data ingestion from Spark DataFrames to feature groups. Feature Store supports batch data ingestion with Spark, using your existing ETL pipeline, on Amazon EMR, GIS, an AWS Glue job, an Amazon SageMaker Processing job, or a SageMaker notebook.

* SageMaker Feature Store offers the following modes for utilizing Feature Store data:
  * **Online Mode**: Retrieves the most recent record with low-latency reads, suitable for high-throughput predictions. Requires a feature group stored in an online store.
  * **Offline Mode**: Stores all historical records, ideal for training and batch inference with large data streams. Requires a feature group stored in an offline store, utilizing Amazon S3 bucket for storage and supporting data retrieval via Athena queries.
  * **Online and Offline Mode**: Integrates both online and offline modes, ensuring synchronization between offline and online datasets to maintain model accuracy. This mode is crucial for preventing discrepancies that could affect model performance.
 
* SageMaker Feature Store organizes features into groups based on parameters or business entities. It supports both streaming and batch inference and allows easy management across ML applications in SageMaker Studio. Different entity types like orders or customers can have their own feature groups, and security is enhanced through tags and permission controls.
* If you trained your model in SageMaker, the model artifacts are saved as a single compressed .tar file in Amazon S3. As with data, versioning in Amazon S3 is a means of keeping multiple variants of an object in the same bucket, such as *.tar.gz files.
* To store and track versions for model training and inference container images, you can use the Amazon Elastic Container Registry (Amazon ECR).
* With a model registry, you can track model versions and their metadata in a central repository. 
The model registry organizes model package groups, each containing multiple versions of a model package. It tracks all model versions trained for a specific problem within a model group. Adding a trained model to the registry creates a new model version within the group, which can be marked as approved or rejected based on predefined acceptance criteria set by the team building the models.
* Each model package in a model group represents a trained model, with its version denoted by a numerical value starting at 1 and incremented with each addition. These versioned model packages in the registry must be linked to a specific model group. A model version must comprise both the model artifacts (trained weights) and the inference code. By defining attributes, you can use tags on both resources and user entities to create tag-based permissions policies. You don’t have to update these policies even as you add new subsidiaries or team members.
* One way to accomplish this standardization is to establish separate repositories for the building and testing, and deployment pipelines. The building and training repository is divided into three main folders:
  *  Algorithms – Typically, data scientists develop the code for each step of the ML pipelines in the algorithms root folder. The steps can be grouped in preprocessing, training, batch inference, and postprocessing (evaluation). In each group, multiple steps can be defined in corresponding subfolders. These folders contain a folder for the unit tests (including optional inputs and outputs), the main functions, and the readme. They also contain a Docker file in case of a custom container need. In addition to main, multiple code files can be hosted in the same folder. Common helper libraries for all of the steps can be hosted in a shared library folder. The data scientists are responsible for the development of the unit tests because they own the logic of the steps. ML engineers are responsible for the error handling enhancement and test coverage recommendation.
  *  ML pipelines – After you develop the source code and tests of each step, the next step is to define the SageMaker pipelines in another root folder. Each ML pipeline definition is placed in a subfolder that contains the .py file. In this example, the training subfolder is shown. This subfolder also contains a JSON or .yaml file for input parameters, such as hyperparameter ranges. A readme file to describe the ML pipelines is necessary.
  *  Notebooks – This folder hosts the origin notebooks that the data scientist used during experimentation.
* The deployment repository consists of three main parts:
  *  Inference configuration – This folder contains the configuration of real-time endpoints or batch inference per development environment, such as instance types.
  *  Application infrastructure (app_infra) – Hosts the source code of the infrastructure that is required to run the inference, if necessary. This code might be a triggering mechanism that uses Amazon EventBridge, Amazon API Gateway, AWS Lambda functions, or SageMaker Pipelines.
  *  Tests – Consists of multiple subfolders depending on the customer testing methodology. As the minimum set of tests, the following tests are suggested: • An integration test (end-to-end run of the inference including application infrastructure) • A stress test (examine edge cases) • ML tests (such as the distribution of confidence scores or probabilities)
 
* 


















