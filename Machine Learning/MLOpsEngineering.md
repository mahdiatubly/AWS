## Introduction to MLOps

* To implement quality control measures at the model registration step. If a model meets baseline performance metrics, it can be registered with a model registry. A model registry can be used as a quality gate. A model registry is a mechanism that is used to:
  * Catalog models for production.
  * Manage model versions.
  * Associate metadata, such as training metrics, with a model.
  * Manage the approval status of a model.
 

## Initial MLOps

* Deep Learning Containers provide optimized environments with TensorFlow and MXNet, Nvidia CUDA (for GPU instances), and Intel MKL (for CPU instances) libraries.
* SageMaker Studio environments do not support the elevated permissions needed to access the Docker daemon for building a Docker image. A separate build environment is needed to extend or build containers. The Amazon SageMaker Studio Image Build Command Line Interface (CLI) lets you build Amazon SageMaker-compatible Docker images directly from your Amazon SageMaker Studio environments.
* The build CLI automatically sets up a reusable build environment that you interact with by using high-level commands. The CLI orchestrates the build workflow using AWS CodeBuild and returns a link to your Amazon Elastic Container Registry (Amazon ECR) image location.
* Accessing the Docker daemon within SageMaker Studio:
  1. Assume execution role
  2. Package directory and upload to S3 bucket
  3. Build the Docker image using AWS CodeBuild
  4. Push the image to the ECR repository and return URI
 
* Training and inference containers in SageMaker must adhere to a specific folder structure. SageMaker-provided containers already have this folder structure in place. Any container that you extend or build new, must also follow this structure. A SageMaker model training container requires a parent directory named ‘/opt/ml’. Within that directory are subdirectories named: code, input, checkpoints, output, and model. SageMaker training uses environment variables to define storage paths.
  * code: You store the custom decision tree algorithm, ‘train.py,’ in this directory. The source container has a root directory containing a dockerfile, build_and_push.sh, and a subdirectory named ‘decision_trees’. This subdirectory contains the custom training algorithm, train.py
  * input: When you run a model training job, the SageMaker container uses SM_INPUT_DIR, which defaults to the /opt/ml/input/ directory. The JSON files that configure the hyperparameters for the algorithm and the resources used for distributed training are stored in the opt/ml/input/config directory. The input directory also contains a subdirectory within the /opt/ml/input/data directory for each channel of training data stored in Amazon S3.
  * checkpoints: If you use checkpointing, Amazon SageMaker saves checkpoints locally in the checkpoint_local_path, /opt/ml/checkpoints, and synchs them to the checkpoint_s3_uri.
  *  outputs: The outputs of a training job are sent from /opt/ml/output/data to the output data uri in S3 as output.tar.gz.
  *  The script must write the model generated by your algorithm to SM_MODEL_DIR, which defaults to /opt/ml/model/. When training is finished, the final model artifact in the /opt/ml/model folder is written to the output data URI in S3 as model.tar.gz.
  *  ../WORKDIR/: Training job operations that are distributed across multiple containers use the WORKDIR/.
 
* SageMaker downloads data from the S3 bucket to the container's file system. When the training job is complete, it uploads the trained model artifact from the local folder to the specified location in S3 and stops the training container.
* When creating your own custom containers for inference, SageMaker requires that you locate your inference code in specific directories. You can host a trained model for inference or batch transform. In both cases, the trained model is loaded into the container in the same folder to which they were written during training.
* You must also add the code required to perform inference using a web application. You place the following files in the /opt/ml/code directory:
   * serve.py – the program started when the container is created for hosting. It launches the gunicorn server which runs multiple instances of the web application defined in predictor.py.
   * predictor.py – the program that implements the web server and the decision tree predictions for this app.
   * webserver.conf – the configuration file for the front end.
   * wrapper.py – is a small wrapper used to invoke the web application.
 
* You can use pre-built containers to deploy your custom models or models that have been trained in a framework outside SageMaker:
  * First you convert the model into a format that is readable by the SageMaker model constructor for the corresponding framework.
  * Second, create a model in SageMaker Inference by pointing to model artifacts stored in Amazon S3 and a container image.
  * Finally, deploy the model to a SageMaker inference endpoint. You will need to choose the number of instances and the type of instances at which to deploy our model.
* The SageMaker Training Toolkit and the SageMaker Inference Toolkit. These two toolkits define the information the container needs to manage training and inference on Amazon SageMaker.
* Domain configurations: An Amazon SageMaker Domain consists of storage resources, a list of authorized users, and a variety of security, application, policy, and Amazon VPC configurations. Users within a Domain can share notebook files and other artifacts with each other. An account can have multiple Domains.
* User profile: A user profile represents a single user within a Domain. It is the main way to reference a user for the purposes of sharing, reporting, and other user-oriented features.
* Spaces: Spaces are used to manage the storage and resource needs of some Amazon SageMaker Studio applications. Each space has a 1:1 relationship with an instance of an application. Spaces can be either private or shared.
  * Private: Private spaces cannot be shared with other users.
  * Shared: Shared spaces are accessible by all users in the domain.
 
* Apps: An app represents an application that supports the reading and execution experience of the user’s notebooks, terminals, and consoles. The type of app can be JupyterServer, KernelGateway, RStudioServerPro, or RSession.
*  SageMaker has different setup options that are optimized for different organizational needs:
  * Quick Setup is the fastest option to start using SageMaker Studio as an individual user. It uses AWS Identity and Access Management (IAM) authentication with auto-generated defaults. Quick setup is helpful for single-user domains or first-time users exploring SageMaker. You can update the account configuration later.
  * Standard Setup requires the most configuration because it has the most options for customization. This option is better for administrators with large user groups. You use this option for VPC configuration, and setting up Single Sign-On or IAM Identity Provider.
  * Templated SageMaker Domain in AWS Service Catalog is used for centralized management of AWS resources. The data scientists in your organization use the AWS Service Catalog to access the SageMaker Domain directly from a preconfigured environment. This option is useful for organizations that need to comply with strict regulatory guidelines, such as HIPPA or Payment Card Industry (PCI). This option activates automated provisioning of environments that are preconfigured to comply with necessary policies.
* SageMaker JumpStart: Example notebooks and pretrained models that you can use fine-tune for your own data.
* Notebook sharing: Share a snapshot of your notebook that team members can copy and work on their own version.  To share your latest version, you must create a new snapshot and then share it as a copy.
*  Shared spaces: A shared space consists of a shared JupyterServer application and a shared directory. Shared spaces facilitate collaboration in real-time. Co-editing is most effective for small groups. 
* SageMaker Projects: Templates you can use for orchestrating ML Ops and continuous integration and delivery (CI/CD) workflows. Orchestration includes managing dependencies, code repositories, build reproducibility, and artifact sharing.
* Encryption - SageMaker uses an AWS Key Management Service (AWS KMS) key to encrypt your Amazon EFS and Amazon Elastic Block Store (Amazon EBS) file systems by default. You can select a customer-managed key instead.
* Authentication – Choose between AWS IAM or AWS IAM Identity Center (successor to AWS SSO). Authenticate with AWS IAM to provide access to the Amazon SageMaker console or AWS CLI. You can use AWS IAM Identity Center to centralize identity management and provide a consistent user sign-in experience.
* Use Amazon SageMaker Role Manager to build and manage persona-based IAM roles for common machine learning needs directly through the Amazon SageMaker console. Amazon SageMaker Role Manager provides three preconfigured role personas with suggested permissions. Permissions are predefined for common ML activities. You can use the activities to create and maintain roles for personas unique to your business needs. These personas include the following:
  * Data Scientist persona – Use this persona to configure permissions to perform general machine learning development and experimentation in a SageMaker environment.
  * MLOps persona – Use this persona to configure permissions for operational activities.
  * SageMaker compute persona – Use this persona to configure permissions to your compute resources, such as the ability to interact with other AWS services.









