## Introduction to MLOps

* To implement quality control measures at the model registration step. If a model meets baseline performance metrics, it can be registered with a model registry. A model registry can be used as a quality gate. A model registry is a mechanism that is used to:
  * Catalog models for production.
  * Manage model versions.
  * Associate metadata, such as training metrics, with a model.
  * Manage the approval status of a model.
 

## Initial MLOps

* Deep Learning Containers provide optimized environments with TensorFlow and MXNet, Nvidia CUDA (for GPU instances), and Intel MKL (for CPU instances) libraries.
* SageMaker Studio environments do not support the elevated permissions needed to access the Docker daemon for building a Docker image. A separate build environment is needed to extend or build containers. The Amazon SageMaker Studio Image Build Command Line Interface (CLI) lets you build Amazon SageMaker-compatible Docker images directly from your Amazon SageMaker Studio environments.
* The build CLI automatically sets up a reusable build environment that you interact with by using high-level commands. The CLI orchestrates the build workflow using AWS CodeBuild and returns a link to your Amazon Elastic Container Registry (Amazon ECR) image location.
* Accessing the Docker daemon within SageMaker Studio:
  1. Assume execution role
  2. Package directory and upload to S3 bucket
  3. Build the Docker image using AWS CodeBuild
  4. Push the image to the ECR repository and return URI
 
* Training and inference containers in SageMaker must adhere to a specific folder structure. SageMaker-provided containers already have this folder structure in place. Any container that you extend or build new, must also follow this structure. A SageMaker model training container requires a parent directory named ‘/opt/ml’. Within that directory are subdirectories named: code, input, checkpoints, output, and model. SageMaker training uses environment variables to define storage paths.
  * code: You store the custom decision tree algorithm, ‘train.py,’ in this directory. The source container has a root directory containing a dockerfile, build_and_push.sh, and a subdirectory named ‘decision_trees’. This subdirectory contains the custom training algorithm, train.py
  * input: When you run a model training job, the SageMaker container uses SM_INPUT_DIR, which defaults to the /opt/ml/input/ directory. The JSON files that configure the hyperparameters for the algorithm and the resources used for distributed training are stored in the opt/ml/input/config directory. The input directory also contains a subdirectory within the /opt/ml/input/data directory for each channel of training data stored in Amazon S3.
  * checkpoints: If you use checkpointing, Amazon SageMaker saves checkpoints locally in the checkpoint_local_path, /opt/ml/checkpoints, and synchs them to the checkpoint_s3_uri.
  *  outputs: The outputs of a training job are sent from /opt/ml/output/data to the output data uri in S3 as output.tar.gz.
  *  The script must write the model generated by your algorithm to SM_MODEL_DIR, which defaults to /opt/ml/model/. When training is finished, the final model artifact in the /opt/ml/model folder is written to the output data URI in S3 as model.tar.gz.
  *  ../WORKDIR/: Training job operations that are distributed across multiple containers use the WORKDIR/.
 
* SageMaker downloads data from the S3 bucket to the container's file system. When the training job is complete, it uploads the trained model artifact from the local folder to the specified location in S3 and stops the training container.
* When creating your own custom containers for inference, SageMaker requires that you locate your inference code in specific directories. You can host a trained model for inference or batch transform. In both cases, the trained model is loaded into the container in the same folder to which they were written during training.
* You must also add the code required to perform inference using a web application. You place the following files in the /opt/ml/code directory:
   * serve.py – the program started when the container is created for hosting. It launches the gunicorn server which runs multiple instances of the web application defined in predictor.py.
   * predictor.py – the program that implements the web server and the decision tree predictions for this app.
   * webserver.conf – the configuration file for the front end.
   * wrapper.py – is a small wrapper used to invoke the web application.
 
* You can use pre-built containers to deploy your custom models or models that have been trained in a framework outside SageMaker:
  * First you convert the model into a format that is readable by the SageMaker model constructor for the corresponding framework.
  * Second, create a model in SageMaker Inference by pointing to model artifacts stored in Amazon S3 and a container image.
  * Finally, deploy the model to a SageMaker inference endpoint. You will need to choose the number of instances and the type of instances at which to deploy our model.
* The SageMaker Training Toolkit and the SageMaker Inference Toolkit. These two toolkits define the information the container needs to manage training and inference on Amazon SageMaker.
* Domain configurations: An Amazon SageMaker Domain consists of storage resources, a list of authorized users, and a variety of security, application, policy, and Amazon VPC configurations. Users within a Domain can share notebook files and other artifacts with each other. An account can have multiple Domains.
* User profile: A user profile represents a single user within a Domain. It is the main way to reference a user for the purposes of sharing, reporting, and other user-oriented features.
* Spaces: Spaces are used to manage the storage and resource needs of some Amazon SageMaker Studio applications. Each space has a 1:1 relationship with an instance of an application. Spaces can be either private or shared.
  * Private: Private spaces cannot be shared with other users.
  * Shared: Shared spaces are accessible by all users in the domain.


  * 





